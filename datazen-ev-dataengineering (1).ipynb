{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14680161,"sourceType":"datasetVersion","datasetId":9378560}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nEV Transition Forecasting - Complete Kaggle Pipeline\n=====================================================\nExecution Order:\n1. Load raw datasets\n2. Join adoption + infrastructure (ONLY)\n3. Feature engineer individual files\n4. Save engineered files with (1) suffix\n5. Create final ML-ready dataset\n6. Data quality validation and export\n\nNO MODEL TRAINING - Data preparation only\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nOUTPUT_DIR = '/kaggle/working/'\nRAW_DATA_PATH = None  # Will be set after kagglehub download\n\n# File mappings\nFILE_MAPPING = {\n    'adoption': 'india_ev_ice_adoption_large.csv',\n    'infra': 'ev_charging_infrastructure_india.csv',\n    'detailed': 'vehicle_registrations_detailed.csv',\n    'battery': 'ev_vehicle_battery_specs_india.csv',\n    'sales': 'ev_ice_market_sales_india.csv'\n}\n\n# ============================================================================\n# SAFETY PRECHECK FUNCTION (PREVENTS INFINITE/NAN BUG)\n# ============================================================================\n\ndef safety_precheck(df, grain_cols, name=\"DataFrame\"):\n    \"\"\"\n    Enforce data quality rules to prevent infinite values and NaN explosions.\n    \n    Rules enforced:\n    1. Deduplicate by grain\n    2. Sort by time\n    3. Remove infinite values\n    4. Validate uniqueness\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"SAFETY PRECHECK: {name}\")\n    print(f\"{'='*60}\")\n    \n    initial_rows = len(df)\n    print(f\"Initial rows: {initial_rows:,}\")\n    \n    # Rule 1: Check for duplicates\n    duplicates = df.duplicated(subset=grain_cols, keep=False).sum()\n    if duplicates > 0:\n        print(f\"‚ö†Ô∏è  WARNING: {duplicates:,} duplicate rows found on grain {grain_cols}\")\n        print(\"   Aggregating to enforce grain...\")\n        # Will be handled by caller\n    \n    # Rule 2: Check sorting\n    if 'year' in df.columns:\n        is_sorted = df.groupby([c for c in grain_cols if c != 'year'])['year'].apply(\n            lambda x: x.is_monotonic_increasing\n        ).all()\n        if not is_sorted:\n            print(\"‚ö†Ô∏è  WARNING: Data not sorted by year\")\n    \n    # Rule 3: Check for infinite values\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    inf_count = np.isinf(df[numeric_cols]).sum().sum()\n    if inf_count > 0:\n        print(f\"‚ö†Ô∏è  WARNING: {inf_count:,} infinite values detected\")\n    \n    # Rule 4: Check for NaN\n    nan_count = df[numeric_cols].isna().sum().sum()\n    print(f\"NaN values: {nan_count:,}\")\n    \n    print(f\"‚úì Precheck complete\")\n    print(f\"{'='*60}\\n\")\n    \n    return df\n\n\n# ============================================================================\n# STEP 0: LOAD RAW DATA\n# ============================================================================\n\ndef load_raw_data(data_path):\n    \"\"\"Load all raw CSV files without modification.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 0: LOADING RAW DATA\")\n    print(\"=\"*60)\n    \n    dfs = {}\n    \n    for key, filename in FILE_MAPPING.items():\n        filepath = os.path.join(data_path, filename)\n        if os.path.exists(filepath):\n            dfs[key] = pd.read_csv(filepath)\n            print(f\"‚úì Loaded {key}: {filename}\")\n            print(f\"  Shape: {dfs[key].shape}\")\n            print(f\"  Columns: {list(dfs[key].columns)[:5]}...\")\n        else:\n            print(f\"‚úó NOT FOUND: {filename}\")\n    \n    return dfs\n\n\n# ============================================================================\n# STEP 1: JOIN ADOPTION + INFRASTRUCTURE (ONLY REQUIRED JOIN)\n# ============================================================================\n\ndef join_adoption_infra(adoption_df, infra_df):\n    \"\"\"\n    LEFT JOIN adoption with infrastructure on (state, year).\n    This is the ONLY join that happens for the ML pipeline.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 1: JOINING ADOPTION + INFRASTRUCTURE\")\n    print(\"=\"*60)\n    \n    # Safety precheck on raw data\n    adoption_df = safety_precheck(\n        adoption_df, \n        ['state', 'year', 'vehicle_segment'],\n        \"Adoption (raw)\"\n    )\n    \n    infra_df = safety_precheck(\n        infra_df,\n        ['state', 'year'],\n        \"Infrastructure (raw)\"\n    )\n    \n    # CRITICAL: Deduplicate before join\n    print(\"\\nDeduplicating adoption data by grain...\")\n    adoption_clean = adoption_df.groupby(\n        ['state', 'year', 'vehicle_segment'],\n        as_index=False\n    ).agg({\n        'ice_vehicle_registrations': 'sum',\n        'ev_vehicle_registrations': 'sum',\n        'charging_stations': 'mean',\n        'avg_ev_subsidy_rs': 'mean',\n        'fuel_price_rs_per_litre': 'mean',\n        'avg_income_index': 'mean'\n    })\n    \n    print(f\"After dedup: {len(adoption_clean):,} rows (from {len(adoption_df):,})\")\n    \n    # CRITICAL ASSERTION: Guarantee grain uniqueness\n    assert adoption_clean.duplicated(['state', 'year', 'vehicle_segment']).sum() == 0, \\\n        \"FATAL: Duplicates still exist after deduplication!\"\n    \n    print(\"\\nDeduplicating infrastructure data by grain...\")\n    infra_clean = infra_df.groupby(\n        ['state', 'year'],\n        as_index=False\n    ).agg({\n        'charging_stations': 'mean',\n        'fast_charger_pct': 'mean',\n        'urban_coverage_pct': 'mean'\n    })\n    \n    print(f\"After dedup: {len(infra_clean):,} rows (from {len(infra_df):,})\")\n    \n    # CRITICAL ASSERTION: Guarantee grain uniqueness\n    assert infra_clean.duplicated(['state', 'year']).sum() == 0, \\\n        \"FATAL: Duplicates still exist in infra after deduplication!\"\n    \n    # Rename infra columns to avoid collision\n    infra_clean = infra_clean.rename(columns={\n        'charging_stations': 'infra_charging_stations',\n        'fast_charger_pct': 'infra_fast_charger_pct',\n        'urban_coverage_pct': 'infra_urban_coverage_pct'\n    })\n    \n    # LEFT JOIN\n    joined = adoption_clean.merge(\n        infra_clean,\n        on=['state', 'year'],\n        how='left',\n        validate='m:1'\n    )\n    \n    print(f\"\\n‚úì Join complete: {len(joined):,} rows\")\n    print(f\"  Null infra values: {joined['infra_charging_stations'].isna().sum()}\")\n    \n    # Use infra_charging_stations if available, else fall back to adoption's\n    joined['charging_stations_final'] = joined['infra_charging_stations'].fillna(\n        joined['charging_stations']\n    )\n    \n    # Sort by time (CRITICAL for lag features)\n    joined = joined.sort_values(['state', 'vehicle_segment', 'year'])\n    \n    return joined\n\n\n# ============================================================================\n# STEP 2A: FEATURE ENGINEERING - ADOPTION TABLE\n# ============================================================================\n\ndef engineer_adoption_features(df):\n    \"\"\"\n    Create adoption-specific features with proper time-series handling.\n    \n    Features created:\n    - EV/ICE shares\n    - Transition metrics\n    - YoY growth rates\n    - Lag features (t-1, t-2)\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 2A: FEATURE ENGINEERING - ADOPTION\")\n    print(\"=\"*60)\n    \n    df = df.copy()\n    \n    # ========== VOLUME & SHARE FEATURES ==========\n    print(\"\\n1. Computing volume & share metrics...\")\n    \n    df['total_registrations'] = df['ev_vehicle_registrations'] + df['ice_vehicle_registrations']\n    df['ev_share'] = df['ev_vehicle_registrations'] / (df['total_registrations'] + 1e-6)\n    df['ice_share'] = df['ice_vehicle_registrations'] / (df['total_registrations'] + 1e-6)\n    \n    # Clip shares to [0, 1]\n    df['ev_share'] = df['ev_share'].clip(0, 1)\n    df['ice_share'] = df['ice_share'].clip(0, 1)\n    \n    print(f\"   EV share range: [{df['ev_share'].min():.4f}, {df['ev_share'].max():.4f}]\")\n    \n    # ========== TEMPORAL / MOMENTUM FEATURES ==========\n    print(\"\\n2. Computing YoY growth rates...\")\n    \n    # Vectorized YoY calculations (safe, fast, correct)\n    df['ev_yoy_growth'] = (\n        df.groupby(['state', 'vehicle_segment'])['ev_vehicle_registrations']\n          .pct_change()\n    )\n    \n    df['ice_yoy_change'] = (\n        df.groupby(['state', 'vehicle_segment'])['ice_vehicle_registrations']\n          .pct_change()\n    )\n    \n    # Clip extreme outliers\n    df['ev_yoy_growth'] = df['ev_yoy_growth'].clip(-1, 5)\n    df['ice_yoy_change'] = df['ice_yoy_change'].clip(-1, 5)\n    \n    # Transition index\n    df['transition_index'] = df['ev_yoy_growth'] - df['ice_yoy_change']\n    \n    # Conversion pressure\n    df['conversion_pressure'] = df['ev_share'] / (df['ice_share'] + 1e-6)\n    \n    print(f\"   Transition index range: [{df['transition_index'].min():.2f}, {df['transition_index'].max():.2f}]\")\n    \n    # ========== LAG FEATURES (CRITICAL) ==========\n    print(\"\\n3. Creating lag features...\")\n    \n    lag_cols = ['ev_share', 'transition_index', 'ev_yoy_growth', 'ice_yoy_change']\n    \n    # Vectorized lag creation (correct and fast)\n    for col in lag_cols:\n        for lag in [1, 2]:\n            new_col = f\"{col}_t-{lag}\"\n            df[new_col] = (\n                df.groupby(['state', 'vehicle_segment'])[col]\n                  .shift(lag)\n            )\n    \n    print(f\"   Created {len(lag_cols) * 2} lag features\")\n    \n    # ========== POLICY & ECONOMIC FEATURES ==========\n    print(\"\\n4. Computing policy & economic changes...\")\n    \n    # Vectorized policy change calculations\n    df['subsidy_yoy_change'] = (\n        df.groupby(['state', 'vehicle_segment'])['avg_ev_subsidy_rs']\n          .pct_change()\n    )\n    \n    df['fuel_price_yoy_change'] = (\n        df.groupby(['state', 'vehicle_segment'])['fuel_price_rs_per_litre']\n          .pct_change()\n    )\n    \n    # Income buckets (quantile-based for non-0-100 range)\n    df['income_bucket'] = pd.qcut(\n        df['avg_income_index'],\n        q=3,\n        labels=['Low', 'Mid', 'High'],\n        duplicates='drop'\n    )\n    \n    # ========== CLEAN BAD VALUES ==========\n    print(\"\\n5. Cleaning infinite/NaN values...\")\n    \n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    \n    initial_rows = len(df)\n    df = df.dropna(subset=['ev_share', 'ev_share_t-1', 'ev_share_t-2'])\n    final_rows = len(df)\n    \n    print(f\"   Rows dropped due to insufficient history: {initial_rows - final_rows:,}\")\n    print(f\"   Final rows: {final_rows:,}\")\n    \n    return df\n\n\n# ============================================================================\n# STEP 2B: FEATURE ENGINEERING - INFRASTRUCTURE TABLE\n# ============================================================================\n\ndef engineer_infra_features(df):\n    \"\"\"\n    Create infrastructure-specific features.\n    \n    Features created:\n    - Normalized infra metrics\n    - Infra growth rates\n    - Quality indices\n    - Lag features\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 2B: FEATURE ENGINEERING - INFRASTRUCTURE\")\n    print(\"=\"*60)\n    \n    df = df.copy()\n    \n    # Use final charging stations\n    if 'charging_stations_final' in df.columns:\n        df['charging_stations'] = df['charging_stations_final']\n    \n    # ========== NORMALIZATION ==========\n    print(\"\\n1. Computing normalized infra metrics...\")\n    \n    # Stations per 10k vehicles\n    df['stations_per_10k_vehicles'] = (\n        df['charging_stations'] / (df['total_registrations'] / 10000 + 1e-6)\n    )\n    \n    # Stations per 1k EVs\n    df['stations_per_1k_ev'] = (\n        df['charging_stations'] / (df['ev_vehicle_registrations'] / 1000 + 1e-6)\n    )\n    \n    # Fast charger index\n    df['fast_charger_index'] = (\n        df['charging_stations'] * (df['infra_fast_charger_pct'] / 100)\n    )\n    \n    print(f\"   Stations per 10k vehicles range: [{df['stations_per_10k_vehicles'].min():.2f}, {df['stations_per_10k_vehicles'].max():.2f}]\")\n    \n    # ========== MOMENTUM FEATURES ==========\n    print(\"\\n2. Computing infra growth rates...\")\n    \n    # Infrastructure is STATE-level, not segment-level\n    df['infra_yoy_growth'] = (\n        df.groupby('state')['charging_stations']\n          .pct_change()\n    )\n    \n    df['fast_charger_pct_change'] = (\n        df.groupby('state')['infra_fast_charger_pct']\n          .pct_change()\n    )\n    \n    df['urban_coverage_yoy_change'] = (\n        df.groupby('state')['infra_urban_coverage_pct']\n          .pct_change()\n    )\n    \n    # ========== LAG FEATURES (CRITICAL FOR CAUSALITY) ==========\n    print(\"\\n3. Creating lagged infra features...\")\n    \n    lag_cols = ['infra_yoy_growth', 'fast_charger_index', 'stations_per_10k_vehicles']\n    \n    # Vectorized lag creation - lagged by (state, segment) for alignment\n    for col in lag_cols:\n        for lag in [1]:\n            new_col = f\"{col}_t-{lag}\"\n            df[new_col] = (\n                df.groupby(['state', 'vehicle_segment'])[col]\n                  .shift(lag)\n            )\n    \n    print(f\"   Created {len(lag_cols)} lagged infra features\")\n    \n    # ========== FLAGS (EXPLAINABILITY) ==========\n    print(\"\\n4. Creating categorical flags...\")\n    \n    # High infra flag\n    infra_median = df['stations_per_10k_vehicles'].median()\n    df['high_infra_flag'] = (df['stations_per_10k_vehicles'] > infra_median).astype(int)\n    \n    # High subsidy flag\n    subsidy_median = df['avg_ev_subsidy_rs'].median()\n    df['high_subsidy_flag'] = (df['avg_ev_subsidy_rs'] > subsidy_median).astype(int)\n    \n    # ========== CLEAN BAD VALUES ==========\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    \n    return df\n\n\n# ============================================================================\n# STEP 2C: FEATURE ENGINEERING - DETAILED REGISTRATIONS\n# ============================================================================\n\ndef engineer_detailed_features(df):\n    \"\"\"\n    Create city-level and OEM-level insights.\n    This stays SEPARATE from ML dataset.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 2C: FEATURE ENGINEERING - DETAILED REGISTRATIONS\")\n    print(\"=\"*60)\n    \n    df = df.copy()\n    \n    # City-level EV share\n    print(\"\\n1. Computing city-level metrics...\")\n    \n    city_metrics = df.groupby(['state', 'city', 'year', 'vehicle_category']).agg({\n        'registrations': 'sum'\n    }).reset_index()\n    \n    city_metrics['total_by_city'] = city_metrics.groupby(\n        ['state', 'city', 'year', 'vehicle_category']\n    )['registrations'].transform('sum')\n    \n    # OEM-level share\n    print(\"2. Computing OEM-level metrics...\")\n    \n    oem_metrics = df.groupby(['manufacturer', 'year', 'fuel_type']).agg({\n        'registrations': 'sum'\n    }).reset_index()\n    \n    print(f\"   City-level records: {len(city_metrics):,}\")\n    print(f\"   OEM-level records: {len(oem_metrics):,}\")\n    \n    return df\n\n\n# ============================================================================\n# STEP 2D: FEATURE ENGINEERING - BATTERY SPECS\n# ============================================================================\n\ndef engineer_battery_features(df):\n    \"\"\"\n    Create segment-level battery context.\n    Used for explanation, not prediction.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 2D: FEATURE ENGINEERING - BATTERY SPECS\")\n    print(\"=\"*60)\n    \n    df = df.copy()\n    \n    # Efficiency metrics\n    print(\"\\n1. Computing battery efficiency metrics...\")\n    \n    df['range_per_kwh'] = df['range_km'] / (df['battery_capacity_kwh'] + 1e-6)\n    df['price_per_kwh'] = df['ex_showroom_price_lakh'] / (df['battery_capacity_kwh'] + 1e-6)\n    df['charging_speed_index'] = df['battery_capacity_kwh'] / (df['charging_time_hrs'] + 1e-6)\n    \n    # Affordability buckets\n    df['affordability_bucket'] = pd.cut(\n        df['ex_showroom_price_lakh'],\n        bins=[0, 10, 20, 100],\n        labels=['Budget', 'Mid', 'Premium']\n    )\n    \n    # Segment-level aggregation\n    segment_metrics = df.groupby('segment').agg({\n        'range_per_kwh': 'mean',\n        'price_per_kwh': 'mean',\n        'charging_speed_index': 'mean',\n        'range_km': 'mean'\n    }).reset_index()\n    \n    print(f\"   Segment-level metrics: {len(segment_metrics)} segments\")\n    \n    return df\n\n\n# ============================================================================\n# STEP 3: CREATE FINAL ML DATASET\n# ============================================================================\n\ndef create_final_ml_dataset(df):\n    \"\"\"\n    Create the single, clean, prediction-ready dataset.\n    \n    Grain: state √ó year √ó vehicle_segment\n    Purpose: Forecast ev_share for t+1, t+2, t+3\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 3: CREATE FINAL ML DATASET\")\n    print(\"=\"*60)\n    \n    df = df.copy()\n    \n    # ========== TARGET CREATION (t+1, t+2, t+3) ==========\n    print(\"\\n1. Creating future targets...\")\n    \n    # Vectorized target creation (safe and fast)\n    for horizon in [1, 2, 3]:\n        target_col = f'ev_share_t+{horizon}'\n        df[target_col] = (\n            df.groupby(['state', 'vehicle_segment'])['ev_share']\n              .shift(-horizon)\n        )\n    \n    print(f\"   Created targets: ev_share_t+1, ev_share_t+2, ev_share_t+3\")\n    \n    # ========== SELECT FINAL COLUMNS ==========\n    print(\"\\n2. Selecting final feature set...\")\n    \n    feature_cols = [\n        # Identifiers\n        'state', 'year', 'vehicle_segment',\n        \n        # Current state\n        'ev_share', 'ice_share', 'total_registrations',\n        'conversion_pressure',\n        \n        # Temporal features\n        'ev_yoy_growth', 'ice_yoy_change', 'transition_index',\n        \n        # Lag features (adoption)\n        'ev_share_t-1', 'ev_share_t-2',\n        'transition_index_t-1',\n        'ev_yoy_growth_t-1', 'ice_yoy_change_t-1',\n        \n        # Infrastructure features (lagged)\n        'infra_yoy_growth_t-1', 'fast_charger_index_t-1',\n        'stations_per_10k_vehicles_t-1',\n        \n        # Policy & economic\n        'avg_ev_subsidy_rs', 'subsidy_yoy_change',\n        'fuel_price_rs_per_litre', 'fuel_price_yoy_change',\n        'avg_income_index',\n        \n        # Flags\n        'high_infra_flag', 'high_subsidy_flag',\n        \n        # Targets\n        'ev_share_t+1', 'ev_share_t+2', 'ev_share_t+3'\n    ]\n    \n    # Filter to existing columns\n    available_cols = [col for col in feature_cols if col in df.columns]\n    missing_cols = [col for col in feature_cols if col not in df.columns]\n    \n    if missing_cols:\n        print(f\"\\n   ‚ö†Ô∏è  Missing columns: {missing_cols}\")\n    \n    final_df = df[available_cols].copy()\n    \n    # ========== FINAL CLEANING ==========\n    print(\"\\n3. Final data cleaning...\")\n    \n    initial_rows = len(final_df)\n    \n    # Drop rows with missing targets\n    final_df = final_df.dropna(subset=['ev_share_t+1', 'ev_share_t+2', 'ev_share_t+3'])\n    \n    # Drop rows with missing critical features\n    final_df = final_df.dropna(subset=['ev_share_t-1', 'ev_share_t-2'])\n    \n    final_rows = len(final_df)\n    \n    print(f\"   Initial rows: {initial_rows:,}\")\n    print(f\"   Final rows: {final_rows:,}\")\n    print(f\"   Rows dropped: {initial_rows - final_rows:,}\")\n    \n    # ========== VALIDATION ==========\n    print(\"\\n4. Data validation...\")\n    \n    print(f\"   Year range: {final_df['year'].min()} - {final_df['year'].max()}\")\n    print(f\"   States: {final_df['state'].nunique()}\")\n    print(f\"   Segments: {final_df['vehicle_segment'].nunique()}\")\n    print(f\"   EV share range: [{final_df['ev_share'].min():.4f}, {final_df['ev_share'].max():.4f}]\")\n    print(f\"   Target t+1 range: [{final_df['ev_share_t+1'].min():.4f}, {final_df['ev_share_t+1'].max():.4f}]\")\n    \n    print(\"\\n   ‚ö†Ô∏è  NOTE: Identifiers (state, year, vehicle_segment) must be\")\n    print(\"            dropped or encoded before model training.\")\n    \n    # Check for data leakage\n    future_cols = [col for col in final_df.columns if 't+' in col and col not in ['ev_share_t+1', 'ev_share_t+2', 'ev_share_t+3']]\n    if future_cols:\n        print(f\"\\n   ‚ö†Ô∏è  WARNING: Potential future leakage detected: {future_cols}\")\n    \n    return final_df\n\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\ndef main():\n    \"\"\"Execute complete pipeline.\"\"\"\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"EV TRANSITION FORECASTING - COMPLETE PIPELINE\")\n    print(\"=\"*60)\n    print(\"\\nExecution steps:\")\n    print(\"0. Load raw data\")\n    print(\"1. Join adoption + infrastructure\")\n    print(\"2. Feature engineer individual files\")\n    print(\"3. Create final ML dataset\")\n    print(\"4. Save all outputs\")\n    print(\"\\n\" + \"=\"*60)\n    \n    # ========== DOWNLOAD DATA ==========\n    print(\"\\nDownloading dataset from Kaggle...\")\n    \n    import kagglehub\n    data_path = kagglehub.dataset_download(\"shubhamindulkar/ev-datasets-for-the-indian-market\")\n    print(f\"Dataset path: {data_path}\")\n    \n    global RAW_DATA_PATH\n    RAW_DATA_PATH = data_path\n    \n    # ========== STEP 0: LOAD ==========\n    raw_dfs = load_raw_data(data_path)\n    \n    if 'adoption' not in raw_dfs or 'infra' not in raw_dfs:\n        print(\"\\n‚ùå ERROR: Core datasets not found!\")\n        return\n    \n    # ========== STEP 1: JOIN ==========\n    joined_df = join_adoption_infra(raw_dfs['adoption'], raw_dfs['infra'])\n    \n    # Save intermediate join\n    join_path = os.path.join(OUTPUT_DIR, 'adoption_infra_joined.csv')\n    joined_df.to_csv(join_path, index=False)\n    print(f\"\\n‚úì Saved: adoption_infra_joined.csv\")\n    \n    # ========== STEP 2: FEATURE ENGINEERING ==========\n    \n    # 2A: Adoption features\n    adoption_eng = engineer_adoption_features(joined_df)\n    adoption_eng_path = os.path.join(OUTPUT_DIR, 'india_ev_ice_adoption_large(1).csv')\n    adoption_eng.to_csv(adoption_eng_path, index=False)\n    print(f\"\\n‚úì Saved: india_ev_ice_adoption_large(1).csv ({len(adoption_eng):,} rows)\")\n    \n    # 2B: Infrastructure features (same DF after both feature sets)\n    infra_eng = engineer_infra_features(adoption_eng)\n    infra_eng_path = os.path.join(OUTPUT_DIR, 'adoption_infra_features(1).csv')\n    infra_eng.to_csv(infra_eng_path, index=False)\n    print(f\"‚úì Saved: adoption_infra_features(1).csv ({len(infra_eng):,} rows)\")\n    \n    # 2C: Detailed registrations (if available)\n    if 'detailed' in raw_dfs:\n        detailed_eng = engineer_detailed_features(raw_dfs['detailed'])\n        detailed_eng_path = os.path.join(OUTPUT_DIR, 'vehicle_registrations_detailed(1).csv')\n        detailed_eng.to_csv(detailed_eng_path, index=False)\n        print(f\"‚úì Saved: vehicle_registrations_detailed(1).csv ({len(detailed_eng):,} rows)\")\n    \n    # 2D: Battery specs (if available)\n    if 'battery' in raw_dfs:\n        battery_eng = engineer_battery_features(raw_dfs['battery'])\n        battery_eng_path = os.path.join(OUTPUT_DIR, 'ev_vehicle_battery_specs_india(1).csv')\n        battery_eng.to_csv(battery_eng_path, index=False)\n        print(f\"‚úì Saved: ev_vehicle_battery_specs_india(1).csv ({len(battery_eng):,} rows)\")\n    \n    # ========== STEP 3: FINAL ML DATASET ==========\n    final_ml_df = create_final_ml_dataset(infra_eng)\n    \n    final_ml_path = os.path.join(OUTPUT_DIR, 'ev_transition_forecast_dataset.csv')\n    final_ml_df.to_csv(final_ml_path, index=False)\n    print(f\"\\n‚úì Saved: ev_transition_forecast_dataset.csv ({len(final_ml_df):,} rows)\")\n    \n    # ========== SUMMARY ==========\n    print(\"\\n\" + \"=\"*60)\n    print(\"PIPELINE COMPLETE\")\n    print(\"=\"*60)\n    print(\"\\nOutput files created:\")\n    print(\"  1. adoption_infra_joined.csv (intermediate)\")\n    print(\"  2. india_ev_ice_adoption_large(1).csv (engineered)\")\n    print(\"  3. adoption_infra_features(1).csv (combined adoption + infra)\")\n    \n    if 'detailed' in raw_dfs:\n        print(\"  4. vehicle_registrations_detailed(1).csv (engineered)\")\n    \n    if 'battery' in raw_dfs:\n        print(\"  5. ev_vehicle_battery_specs_india(1).csv (engineered)\")\n    \n    print(f\"\\n  üî• FINAL ML DATASET: ev_transition_forecast_dataset.csv\")\n    print(f\"     - {len(final_ml_df):,} rows\")\n    print(f\"     - {len(final_ml_df.columns)} features\")\n    print(f\"     - Ready for model training\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Next steps:\")\n    print(\"  - Review engineered files\")\n    print(\"  - Validate data quality\")\n    print(\"  - Build prediction models (separate notebook)\")\n    print(\"=\"*60)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T05:41:36.401226Z","iopub.execute_input":"2026-01-31T05:41:36.401639Z","iopub.status.idle":"2026-01-31T05:41:38.448029Z","shell.execute_reply.started":"2026-01-31T05:41:36.401608Z","shell.execute_reply":"2026-01-31T05:41:38.447026Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nEV TRANSITION FORECASTING - COMPLETE PIPELINE\n============================================================\n\nExecution steps:\n0. Load raw data\n1. Join adoption + infrastructure\n2. Feature engineer individual files\n3. Create final ML dataset\n4. Save all outputs\n\n============================================================\n\nDownloading dataset from Kaggle...\nDataset path: /kaggle/input/ev-datasets-for-the-indian-market\n\n============================================================\nSTEP 0: LOADING RAW DATA\n============================================================\n‚úì Loaded adoption: india_ev_ice_adoption_large.csv\n  Shape: (120000, 9)\n  Columns: ['state', 'year', 'vehicle_segment', 'ice_vehicle_registrations', 'ev_vehicle_registrations']...\n‚úì Loaded infra: ev_charging_infrastructure_india.csv\n  Shape: (162, 5)\n  Columns: ['state', 'year', 'charging_stations', 'fast_charger_pct', 'urban_coverage_pct']...\n‚úì Loaded detailed: vehicle_registrations_detailed.csv\n  Shape: (9072, 9)\n  Columns: ['year', 'month', 'state', 'city', 'vehicle_category']...\n‚úì Loaded battery: ev_vehicle_battery_specs_india.csv\n  Shape: (6, 6)\n  Columns: ['vehicle_model', 'segment', 'battery_capacity_kwh', 'range_km', 'charging_time_hrs']...\n‚úì Loaded sales: ev_ice_market_sales_india.csv\n  Shape: (120000, 5)\n  Columns: ['state', 'year', 'vehicle_segment', 'ice_vehicle_registrations', 'ev_vehicle_registrations']...\n\n============================================================\nSTEP 1: JOINING ADOPTION + INFRASTRUCTURE\n============================================================\n\n============================================================\nSAFETY PRECHECK: Adoption (raw)\n============================================================\nInitial rows: 120,000\n‚ö†Ô∏è  WARNING: 120,000 duplicate rows found on grain ['state', 'year', 'vehicle_segment']\n   Aggregating to enforce grain...\n‚ö†Ô∏è  WARNING: Data not sorted by year\nNaN values: 0\n‚úì Precheck complete\n============================================================\n\n\n============================================================\nSAFETY PRECHECK: Infrastructure (raw)\n============================================================\nInitial rows: 162\nNaN values: 0\n‚úì Precheck complete\n============================================================\n\n\nDeduplicating adoption data by grain...\nAfter dedup: 486 rows (from 120,000)\n\nDeduplicating infrastructure data by grain...\nAfter dedup: 162 rows (from 162)\n\n‚úì Join complete: 486 rows\n  Null infra values: 0\n\n‚úì Saved: adoption_infra_joined.csv\n\n============================================================\nSTEP 2A: FEATURE ENGINEERING - ADOPTION\n============================================================\n\n1. Computing volume & share metrics...\n   EV share range: [0.1158, 0.1446]\n\n2. Computing YoY growth rates...\n   Transition index range: [-0.16, 0.19]\n\n3. Creating lag features...\n   Created 8 lag features\n\n4. Computing policy & economic changes...\n\n5. Cleaning infinite/NaN values...\n   Rows dropped due to insufficient history: 108\n   Final rows: 378\n\n‚úì Saved: india_ev_ice_adoption_large(1).csv (378 rows)\n\n============================================================\nSTEP 2B: FEATURE ENGINEERING - INFRASTRUCTURE\n============================================================\n\n1. Computing normalized infra metrics...\n   Stations per 10k vehicles range: [0.01, 0.89]\n\n2. Computing infra growth rates...\n\n3. Creating lagged infra features...\n   Created 3 lagged infra features\n\n4. Creating categorical flags...\n‚úì Saved: adoption_infra_features(1).csv (378 rows)\n\n============================================================\nSTEP 2C: FEATURE ENGINEERING - DETAILED REGISTRATIONS\n============================================================\n\n1. Computing city-level metrics...\n2. Computing OEM-level metrics...\n   City-level records: 168\n   OEM-level records: 48\n‚úì Saved: vehicle_registrations_detailed(1).csv (9,072 rows)\n\n============================================================\nSTEP 2D: FEATURE ENGINEERING - BATTERY SPECS\n============================================================\n\n1. Computing battery efficiency metrics...\n   Segment-level metrics: 2 segments\n‚úì Saved: ev_vehicle_battery_specs_india(1).csv (6 rows)\n\n============================================================\nSTEP 3: CREATE FINAL ML DATASET\n============================================================\n\n1. Creating future targets...\n   Created targets: ev_share_t+1, ev_share_t+2, ev_share_t+3\n\n2. Selecting final feature set...\n\n3. Final data cleaning...\n   Initial rows: 378\n   Final rows: 216\n   Rows dropped: 162\n\n4. Data validation...\n   Year range: 2018 - 2021\n   States: 18\n   Segments: 3\n   EV share range: [0.1158, 0.1446]\n   Target t+1 range: [0.1158, 0.1446]\n\n   ‚ö†Ô∏è  NOTE: Identifiers (state, year, vehicle_segment) must be\n            dropped or encoded before model training.\n\n‚úì Saved: ev_transition_forecast_dataset.csv (216 rows)\n\n============================================================\nPIPELINE COMPLETE\n============================================================\n\nOutput files created:\n  1. adoption_infra_joined.csv (intermediate)\n  2. india_ev_ice_adoption_large(1).csv (engineered)\n  3. adoption_infra_features(1).csv (combined adoption + infra)\n  4. vehicle_registrations_detailed(1).csv (engineered)\n  5. ev_vehicle_battery_specs_india(1).csv (engineered)\n\n  üî• FINAL ML DATASET: ev_transition_forecast_dataset.csv\n     - 216 rows\n     - 28 features\n     - Ready for model training\n\n============================================================\nNext steps:\n  - Review engineered files\n  - Validate data quality\n  - Build prediction models (separate notebook)\n============================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nEV Transition Forecasting - Model Training & Evaluation\n========================================================\n6 Models + Ensemble for t+1, t+2, t+3 horizon predictions\n\nModels:\n1. Linear Regression (Baseline)\n2. Ridge Regression (L2 regularization)\n3. Lasso Regression (L1 regularization, feature selection)\n4. ElasticNet (L1+L2, best of both)\n5. Random Forest (Tree ensemble)\n6. LightGBM (Gradient boosting)\n7. ENSEMBLE (Weighted average of best 3)\n\nExecution: python ev_model_training.py\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n\n# Models\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\n\nimport pickle\nimport os\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nINPUT_FILE = 'ev_transition_forecast_dataset.csv'\nOUTPUT_DIR = 'models/'\nPREDICTIONS_DIR = 'predictions/'\n\n# Create directories\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(PREDICTIONS_DIR, exist_ok=True)\n\n# Random seed for reproducibility\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\n# ============================================================================\n# DATA LOADING & PREPROCESSING\n# ============================================================================\n\ndef load_and_prepare_data(filepath):\n    \"\"\"Load dataset and prepare for modeling.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 1: LOADING & PREPROCESSING DATA\")\n    print(\"=\"*60)\n    \n    df = pd.read_csv(filepath)\n    print(f\"‚úì Loaded dataset: {df.shape}\")\n    \n    # Separate features, identifiers, and targets\n    identifier_cols = ['state', 'year', 'vehicle_segment']\n    target_cols = ['ev_share_t+1', 'ev_share_t+2', 'ev_share_t+3']\n    \n    # Feature columns (everything except identifiers and targets)\n    feature_cols = [col for col in df.columns \n                    if col not in identifier_cols + target_cols]\n    \n    print(f\"\\n‚úì Identifiers: {len(identifier_cols)} - {identifier_cols}\")\n    print(f\"‚úì Features: {len(feature_cols)}\")\n    print(f\"‚úì Targets: {len(target_cols)} - {target_cols}\")\n    \n    return df, feature_cols, identifier_cols, target_cols\n\n\ndef encode_categorical(df, feature_cols):\n    \"\"\"Encode categorical variables.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 2: ENCODING CATEGORICAL VARIABLES\")\n    print(\"=\"*60)\n    \n    df = df.copy()\n    encoders = {}\n    \n    # Encode state\n    if 'state' in df.columns:\n        le_state = LabelEncoder()\n        df['state_encoded'] = le_state.fit_transform(df['state'])\n        encoders['state'] = le_state\n        print(f\"‚úì Encoded 'state': {df['state'].nunique()} unique values\")\n    \n    # Encode vehicle_segment\n    if 'vehicle_segment' in df.columns:\n        le_segment = LabelEncoder()\n        df['segment_encoded'] = le_segment.fit_transform(df['vehicle_segment'])\n        encoders['vehicle_segment'] = le_segment\n        print(f\"‚úì Encoded 'vehicle_segment': {df['vehicle_segment'].nunique()} unique values\")\n    \n    # Update feature columns\n    new_features = feature_cols + ['state_encoded', 'segment_encoded']\n    \n    return df, new_features, encoders\n\n\ndef handle_missing_values(df, feature_cols):\n    \"\"\"Handle missing values in features.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 3: HANDLING MISSING VALUES\")\n    print(\"=\"*60)\n    \n    df = df.copy()\n    \n    # Check for missing values\n    missing = df[feature_cols].isna().sum()\n    missing_cols = missing[missing > 0].sort_values(ascending=False)\n    \n    if len(missing_cols) > 0:\n        print(f\"\\n‚ö†Ô∏è  Columns with missing values:\")\n        for col, count in missing_cols.items():\n            pct = count / len(df) * 100\n            print(f\"   {col}: {count} ({pct:.1f}%)\")\n        \n        # Strategy: Fill with median for numerical columns\n        for col in missing_cols.index:\n            if col in df[feature_cols].columns:\n                median_val = df[col].median()\n                df[col].fillna(median_val, inplace=True)\n                print(f\"   ‚Üí Filled '{col}' with median: {median_val:.4f}\")\n        \n        print(f\"\\n‚úì All missing values handled\")\n    else:\n        print(\"‚úì No missing values detected in features\")\n    \n    return df\n\n\ndef create_train_test_split(df, feature_cols, target_cols):\n    \"\"\"Create time-based train/test split.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 4: TRAIN/TEST SPLIT (TIME-BASED)\")\n    print(\"=\"*60)\n    \n    # Time-based split: 2018-2019 train, 2020-2021 test\n    train_years = [2018, 2019]\n    test_years = [2020, 2021]\n    \n    train_mask = df['year'].isin(train_years)\n    test_mask = df['year'].isin(test_years)\n    \n    train_df = df[train_mask].copy()\n    test_df = df[test_mask].copy()\n    \n    print(f\"‚úì Train set: {len(train_df)} records (years {train_years})\")\n    print(f\"‚úì Test set: {len(test_df)} records (years {test_years})\")\n    print(f\"‚úì Train/Test ratio: {len(train_df)/len(test_df):.2f}\")\n    \n    # Extract features and targets\n    X_train = train_df[feature_cols]\n    X_test = test_df[feature_cols]\n    \n    targets = {}\n    for target in target_cols:\n        targets[target] = {\n            'y_train': train_df[target],\n            'y_test': test_df[target]\n        }\n    \n    # Keep identifiers for later analysis\n    train_ids = train_df[['state', 'year', 'vehicle_segment']]\n    test_ids = test_df[['state', 'year', 'vehicle_segment']]\n    \n    return X_train, X_test, targets, train_ids, test_ids\n\n\ndef scale_features(X_train, X_test):\n    \"\"\"Scale features for models that benefit from normalization.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 5: FEATURE SCALING\")\n    print(\"=\"*60)\n    \n    scaler = StandardScaler()\n    \n    X_train_scaled = pd.DataFrame(\n        scaler.fit_transform(X_train),\n        columns=X_train.columns,\n        index=X_train.index\n    )\n    \n    X_test_scaled = pd.DataFrame(\n        scaler.transform(X_test),\n        columns=X_test.columns,\n        index=X_test.index\n    )\n    \n    print(f\"‚úì Features scaled (mean=0, std=1)\")\n    print(f\"   Train shape: {X_train_scaled.shape}\")\n    print(f\"   Test shape: {X_test_scaled.shape}\")\n    \n    return X_train_scaled, X_test_scaled, scaler\n\n\n# ============================================================================\n# MODEL DEFINITIONS\n# ============================================================================\n\ndef get_models():\n    \"\"\"Define all models to train.\"\"\"\n    models = {\n        '1_LinearRegression': {\n            'model': LinearRegression(),\n            'needs_scaling': True,\n            'description': 'Baseline - OLS regression'\n        },\n        '2_Ridge': {\n            'model': Ridge(alpha=1.0, random_state=RANDOM_STATE),\n            'needs_scaling': True,\n            'description': 'L2 regularization'\n        },\n        '3_Lasso': {\n            'model': Lasso(alpha=0.001, random_state=RANDOM_STATE, max_iter=5000),\n            'needs_scaling': True,\n            'description': 'L1 regularization + feature selection'\n        },\n        '4_ElasticNet': {\n            'model': ElasticNet(alpha=0.001, l1_ratio=0.5, random_state=RANDOM_STATE, max_iter=5000),\n            'needs_scaling': True,\n            'description': 'L1 + L2 regularization'\n        },\n        '5_RandomForest': {\n            'model': RandomForestRegressor(\n                n_estimators=100,\n                max_depth=5,\n                min_samples_split=5,\n                min_samples_leaf=2,\n                random_state=RANDOM_STATE,\n                n_jobs=-1\n            ),\n            'needs_scaling': False,\n            'description': 'Tree ensemble - handles non-linearity'\n        },\n        '6_LightGBM': {\n            'model': LGBMRegressor(\n                n_estimators=100,\n                max_depth=5,\n                learning_rate=0.05,\n                num_leaves=31,\n                random_state=RANDOM_STATE,\n                verbose=-1\n            ),\n            'needs_scaling': False,\n            'description': 'Gradient boosting - fast & accurate'\n        }\n    }\n    \n    return models\n\n\n# ============================================================================\n# TRAINING & EVALUATION\n# ============================================================================\n\ndef evaluate_model(y_true, y_pred, model_name, target_name):\n    \"\"\"Calculate evaluation metrics.\"\"\"\n    mae = mean_absolute_error(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)  # Compatible with all scikit-learn versions\n    r2 = r2_score(y_true, y_pred)\n    \n    # MAPE (handle division by zero)\n    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\n    \n    metrics = {\n        'model': model_name,\n        'target': target_name,\n        'MAE': mae,\n        'RMSE': rmse,\n        'R2': r2,\n        'MAPE': mape\n    }\n    \n    return metrics\n\n\ndef train_all_models(X_train, X_train_scaled, X_test, X_test_scaled, targets):\n    \"\"\"Train all models for all target horizons.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 6: TRAINING MODELS\")\n    print(\"=\"*60)\n    \n    models = get_models()\n    all_results = []\n    trained_models = {}\n    all_predictions = {}\n    \n    target_names = list(targets.keys())\n    \n    for target_name in target_names:\n        print(f\"\\n{'='*60}\")\n        print(f\"TARGET: {target_name}\")\n        print(f\"{'='*60}\")\n        \n        y_train = targets[target_name]['y_train']\n        y_test = targets[target_name]['y_test']\n        \n        target_predictions = {}\n        \n        for model_name, model_config in models.items():\n            print(f\"\\n{model_name}: {model_config['description']}\")\n            \n            # Select appropriate data\n            if model_config['needs_scaling']:\n                X_tr = X_train_scaled\n                X_te = X_test_scaled\n            else:\n                X_tr = X_train\n                X_te = X_test\n            \n            # Train model\n            model = model_config['model']\n            model.fit(X_tr, y_train)\n            \n            # Predictions\n            y_pred_train = model.predict(X_tr)\n            y_pred_test = model.predict(X_te)\n            \n            # Evaluate\n            train_metrics = evaluate_model(y_train, y_pred_train, model_name, target_name)\n            test_metrics = evaluate_model(y_test, y_pred_test, model_name, target_name)\n            \n            print(f\"   Train - MAE: {train_metrics['MAE']:.6f} | R¬≤: {train_metrics['R2']:.4f}\")\n            print(f\"   Test  - MAE: {test_metrics['MAE']:.6f} | R¬≤: {test_metrics['R2']:.4f}\")\n            \n            # Store results\n            train_metrics['split'] = 'train'\n            test_metrics['split'] = 'test'\n            all_results.append(train_metrics)\n            all_results.append(test_metrics)\n            \n            # Store model\n            model_key = f\"{model_name}_{target_name}\"\n            trained_models[model_key] = {\n                'model': model,\n                'scaler_used': model_config['needs_scaling']\n            }\n            \n            # Store predictions\n            target_predictions[model_name] = y_pred_test\n        \n        all_predictions[target_name] = target_predictions\n    \n    return trained_models, all_results, all_predictions\n\n\n# ============================================================================\n# ENSEMBLE MODEL\n# ============================================================================\n\ndef create_ensemble(all_predictions, targets):\n    \"\"\"Create ensemble model from top 3 performers.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 7: CREATING ENSEMBLE MODEL\")\n    print(\"=\"*60)\n    \n    ensemble_predictions = {}\n    ensemble_results = []\n    \n    for target_name, predictions_dict in all_predictions.items():\n        print(f\"\\n{target_name}:\")\n        \n        y_test = targets[target_name]['y_test']\n        \n        # Calculate MAE for each model\n        model_scores = {}\n        for model_name, y_pred in predictions_dict.items():\n            mae = mean_absolute_error(y_test, y_pred)\n            model_scores[model_name] = mae\n        \n        # Select top 3 models\n        top_3 = sorted(model_scores.items(), key=lambda x: x[1])[:3]\n        print(f\"   Top 3 models:\")\n        for i, (model_name, mae) in enumerate(top_3, 1):\n            print(f\"      {i}. {model_name}: MAE = {mae:.6f}\")\n        \n        # Weighted ensemble (inverse MAE)\n        weights = [1/mae for _, mae in top_3]\n        weight_sum = sum(weights)\n        weights = [w/weight_sum for w in weights]\n        \n        # Create ensemble prediction\n        ensemble_pred = np.zeros_like(y_test)\n        for (model_name, _), weight in zip(top_3, weights):\n            ensemble_pred += weight * predictions_dict[model_name]\n        \n        # Evaluate ensemble\n        ensemble_metrics = evaluate_model(y_test, ensemble_pred, 'ENSEMBLE', target_name)\n        ensemble_metrics['split'] = 'test'\n        ensemble_results.append(ensemble_metrics)\n        \n        print(f\"   ENSEMBLE - MAE: {ensemble_metrics['MAE']:.6f} | R¬≤: {ensemble_metrics['R2']:.4f}\")\n        \n        ensemble_predictions[target_name] = ensemble_pred\n    \n    return ensemble_predictions, ensemble_results\n\n\n# ============================================================================\n# SAVE MODELS & RESULTS\n# ============================================================================\n\ndef save_models(trained_models, scaler, encoders):\n    \"\"\"Save all trained models.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 8: SAVING MODELS\")\n    print(\"=\"*60)\n    \n    for model_name, model_data in trained_models.items():\n        filepath = os.path.join(OUTPUT_DIR, f\"{model_name}.pkl\")\n        with open(filepath, 'wb') as f:\n            pickle.dump(model_data['model'], f)\n        print(f\"‚úì Saved: {filepath}\")\n    \n    # Save scaler\n    scaler_path = os.path.join(OUTPUT_DIR, 'scaler.pkl')\n    with open(scaler_path, 'wb') as f:\n        pickle.dump(scaler, f)\n    print(f\"‚úì Saved: {scaler_path}\")\n    \n    # Save encoders\n    encoders_path = os.path.join(OUTPUT_DIR, 'encoders.pkl')\n    with open(encoders_path, 'wb') as f:\n        pickle.dump(encoders, f)\n    print(f\"‚úì Saved: {encoders_path}\")\n\n\ndef save_results(all_results, ensemble_results, test_ids, all_predictions, ensemble_predictions, targets):\n    \"\"\"Save evaluation results and predictions.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 9: SAVING RESULTS\")\n    print(\"=\"*60)\n    \n    # Combine all results\n    results_df = pd.DataFrame(all_results + ensemble_results)\n    results_path = os.path.join(PREDICTIONS_DIR, 'model_evaluation_results.csv')\n    results_df.to_csv(results_path, index=False)\n    print(f\"‚úì Saved evaluation results: {results_path}\")\n    \n    # Save predictions for each target\n    for target_name in all_predictions.keys():\n        pred_df = test_ids.copy()\n        pred_df['y_true'] = targets[target_name]['y_test'].values\n        \n        # Add all model predictions\n        for model_name, y_pred in all_predictions[target_name].items():\n            pred_df[f'pred_{model_name}'] = y_pred\n        \n        # Add ensemble\n        pred_df['pred_ENSEMBLE'] = ensemble_predictions[target_name]\n        \n        # Save\n        pred_path = os.path.join(PREDICTIONS_DIR, f'predictions_{target_name}.csv')\n        pred_df.to_csv(pred_path, index=False)\n        print(f\"‚úì Saved predictions: {pred_path}\")\n\n\ndef print_final_summary(all_results, ensemble_results):\n    \"\"\"Print final performance summary.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"FINAL PERFORMANCE SUMMARY\")\n    print(\"=\"*60)\n    \n    results_df = pd.DataFrame(all_results + ensemble_results)\n    \n    # Test set performance only\n    test_results = results_df[results_df['split'] == 'test'].copy()\n    \n    for target in test_results['target'].unique():\n        print(f\"\\n{target}:\")\n        target_df = test_results[test_results['target'] == target].sort_values('MAE')\n        \n        print(f\"\\n{'Model':<20} {'MAE':>10} {'RMSE':>10} {'R¬≤':>10} {'MAPE':>10}\")\n        print(\"-\" * 62)\n        \n        for _, row in target_df.iterrows():\n            print(f\"{row['model']:<20} {row['MAE']:>10.6f} {row['RMSE']:>10.6f} {row['R2']:>10.4f} {row['MAPE']:>9.2f}%\")\n        \n        # Highlight best\n        best = target_df.iloc[0]\n        print(f\"\\nüèÜ BEST: {best['model']} (MAE: {best['MAE']:.6f})\")\n\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\ndef main():\n    \"\"\"Execute complete model training pipeline.\"\"\"\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"EV TRANSITION FORECASTING - MODEL TRAINING\")\n    print(\"=\"*60)\n    print(\"\\nModels to train:\")\n    print(\"  1. Linear Regression (Baseline)\")\n    print(\"  2. Ridge Regression (L2)\")\n    print(\"  3. Lasso Regression (L1)\")\n    print(\"  4. ElasticNet (L1+L2)\")\n    print(\"  5. Random Forest\")\n    print(\"  6. LightGBM\")\n    print(\"  7. ENSEMBLE (Top 3)\")\n    print(\"\\nTargets: ev_share_t+1, ev_share_t+2, ev_share_t+3\")\n    print(\"=\"*60)\n    \n    # Load data\n    df, feature_cols, identifier_cols, target_cols = load_and_prepare_data(INPUT_FILE)\n    \n    # Encode categorical\n    df, feature_cols, encoders = encode_categorical(df, feature_cols)\n    \n    # Handle missing values\n    df = handle_missing_values(df, feature_cols)\n    \n    # Train/test split\n    X_train, X_test, targets, train_ids, test_ids = create_train_test_split(\n        df, feature_cols, target_cols\n    )\n    \n    # Scale features\n    X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)\n    \n    # Train all models\n    trained_models, all_results, all_predictions = train_all_models(\n        X_train, X_train_scaled, X_test, X_test_scaled, targets\n    )\n    \n    # Create ensemble\n    ensemble_predictions, ensemble_results = create_ensemble(all_predictions, targets)\n    \n    # Save everything\n    save_models(trained_models, scaler, encoders)\n    save_results(all_results, ensemble_results, test_ids, all_predictions, \n                 ensemble_predictions, targets)\n    \n    # Final summary\n    print_final_summary(all_results, ensemble_results)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING COMPLETE ‚úÖ\")\n    print(\"=\"*60)\n    print(f\"\\nOutputs saved to:\")\n    print(f\"  Models: {OUTPUT_DIR}\")\n    print(f\"  Predictions: {PREDICTIONS_DIR}\")\n    print(\"\\n\" + \"=\"*60)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T06:02:20.522448Z","iopub.execute_input":"2026-01-31T06:02:20.523106Z","iopub.status.idle":"2026-01-31T06:02:21.646989Z","shell.execute_reply.started":"2026-01-31T06:02:20.523075Z","shell.execute_reply":"2026-01-31T06:02:21.646063Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nEV TRANSITION FORECASTING - MODEL TRAINING\n============================================================\n\nModels to train:\n  1. Linear Regression (Baseline)\n  2. Ridge Regression (L2)\n  3. Lasso Regression (L1)\n  4. ElasticNet (L1+L2)\n  5. Random Forest\n  6. LightGBM\n  7. ENSEMBLE (Top 3)\n\nTargets: ev_share_t+1, ev_share_t+2, ev_share_t+3\n============================================================\n\n============================================================\nSTEP 1: LOADING & PREPROCESSING DATA\n============================================================\n‚úì Loaded dataset: (216, 28)\n\n‚úì Identifiers: 3 - ['state', 'year', 'vehicle_segment']\n‚úì Features: 22\n‚úì Targets: 3 - ['ev_share_t+1', 'ev_share_t+2', 'ev_share_t+3']\n\n============================================================\nSTEP 2: ENCODING CATEGORICAL VARIABLES\n============================================================\n‚úì Encoded 'state': 18 unique values\n‚úì Encoded 'vehicle_segment': 3 unique values\n\n============================================================\nSTEP 3: HANDLING MISSING VALUES\n============================================================\n\n‚ö†Ô∏è  Columns with missing values:\n   infra_yoy_growth_t-1: 72 (33.3%)\n   fast_charger_index_t-1: 54 (25.0%)\n   stations_per_10k_vehicles_t-1: 54 (25.0%)\n   ‚Üí Filled 'infra_yoy_growth_t-1' with median: 0.1538\n   ‚Üí Filled 'fast_charger_index_t-1' with median: 2.5743\n   ‚Üí Filled 'stations_per_10k_vehicles_t-1' with median: 0.2559\n\n‚úì All missing values handled\n\n============================================================\nSTEP 4: TRAIN/TEST SPLIT (TIME-BASED)\n============================================================\n‚úì Train set: 108 records (years [2018, 2019])\n‚úì Test set: 108 records (years [2020, 2021])\n‚úì Train/Test ratio: 1.00\n\n============================================================\nSTEP 5: FEATURE SCALING\n============================================================\n‚úì Features scaled (mean=0, std=1)\n   Train shape: (108, 24)\n   Test shape: (108, 24)\n\n============================================================\nSTEP 6: TRAINING MODELS\n============================================================\n\n============================================================\nTARGET: ev_share_t+1\n============================================================\n\n1_LinearRegression: Baseline - OLS regression\n   Train - MAE: 0.003390 | R¬≤: 0.1397\n   Test  - MAE: 0.004208 | R¬≤: -0.3481\n\n2_Ridge: L2 regularization\n   Train - MAE: 0.003441 | R¬≤: 0.1296\n   Test  - MAE: 0.004167 | R¬≤: -0.2851\n\n3_Lasso: L1 regularization + feature selection\n   Train - MAE: 0.003648 | R¬≤: 0.0000\n   Test  - MAE: 0.003649 | R¬≤: -0.0366\n\n4_ElasticNet: L1 + L2 regularization\n   Train - MAE: 0.003614 | R¬≤: 0.0196\n   Test  - MAE: 0.003742 | R¬≤: -0.0692\n\n5_RandomForest: Tree ensemble - handles non-linearity\n   Train - MAE: 0.002329 | R¬≤: 0.6190\n   Test  - MAE: 0.003846 | R¬≤: -0.1349\n\n6_LightGBM: Gradient boosting - fast & accurate\n   Train - MAE: 0.002399 | R¬≤: 0.5938\n   Test  - MAE: 0.003920 | R¬≤: -0.2001\n\n============================================================\nTARGET: ev_share_t+2\n============================================================\n\n1_LinearRegression: Baseline - OLS regression\n   Train - MAE: 0.003579 | R¬≤: 0.1814\n   Test  - MAE: 0.004772 | R¬≤: -0.8830\n\n2_Ridge: L2 regularization\n   Train - MAE: 0.003683 | R¬≤: 0.1432\n   Test  - MAE: 0.004488 | R¬≤: -0.7323\n\n3_Lasso: L1 regularization + feature selection\n   Train - MAE: 0.003925 | R¬≤: 0.0000\n   Test  - MAE: 0.003676 | R¬≤: -0.0074\n\n4_ElasticNet: L1 + L2 regularization\n   Train - MAE: 0.003847 | R¬≤: 0.0350\n   Test  - MAE: 0.003685 | R¬≤: -0.0159\n\n5_RandomForest: Tree ensemble - handles non-linearity\n   Train - MAE: 0.002228 | R¬≤: 0.6680\n   Test  - MAE: 0.004133 | R¬≤: -0.3102\n\n6_LightGBM: Gradient boosting - fast & accurate\n   Train - MAE: 0.002635 | R¬≤: 0.5372\n   Test  - MAE: 0.003972 | R¬≤: -0.2410\n\n============================================================\nTARGET: ev_share_t+3\n============================================================\n\n1_LinearRegression: Baseline - OLS regression\n   Train - MAE: 0.003348 | R¬≤: 0.1478\n   Test  - MAE: 0.004359 | R¬≤: -0.4350\n\n2_Ridge: L2 regularization\n   Train - MAE: 0.003356 | R¬≤: 0.1238\n   Test  - MAE: 0.004085 | R¬≤: -0.1970\n\n3_Lasso: L1 regularization + feature selection\n   Train - MAE: 0.003549 | R¬≤: 0.0053\n   Test  - MAE: 0.003736 | R¬≤: -0.0021\n\n4_ElasticNet: L1 + L2 regularization\n   Train - MAE: 0.003502 | R¬≤: 0.0451\n   Test  - MAE: 0.003759 | R¬≤: -0.0235\n\n5_RandomForest: Tree ensemble - handles non-linearity\n   Train - MAE: 0.002122 | R¬≤: 0.6653\n   Test  - MAE: 0.003844 | R¬≤: -0.0701\n\n6_LightGBM: Gradient boosting - fast & accurate\n   Train - MAE: 0.002321 | R¬≤: 0.5771\n   Test  - MAE: 0.003897 | R¬≤: -0.0836\n\n============================================================\nSTEP 7: CREATING ENSEMBLE MODEL\n============================================================\n\nev_share_t+1:\n   Top 3 models:\n      1. 3_Lasso: MAE = 0.003649\n      2. 4_ElasticNet: MAE = 0.003742\n      3. 5_RandomForest: MAE = 0.003846\n   ENSEMBLE - MAE: 0.003693 | R¬≤: -0.0590\n\nev_share_t+2:\n   Top 3 models:\n      1. 3_Lasso: MAE = 0.003676\n      2. 4_ElasticNet: MAE = 0.003685\n      3. 6_LightGBM: MAE = 0.003972\n   ENSEMBLE - MAE: 0.003713 | R¬≤: -0.0296\n\nev_share_t+3:\n   Top 3 models:\n      1. 3_Lasso: MAE = 0.003736\n      2. 4_ElasticNet: MAE = 0.003759\n      3. 5_RandomForest: MAE = 0.003844\n   ENSEMBLE - MAE: 0.003763 | R¬≤: -0.0196\n\n============================================================\nSTEP 8: SAVING MODELS\n============================================================\n‚úì Saved: models/1_LinearRegression_ev_share_t+1.pkl\n‚úì Saved: models/2_Ridge_ev_share_t+1.pkl\n‚úì Saved: models/3_Lasso_ev_share_t+1.pkl\n‚úì Saved: models/4_ElasticNet_ev_share_t+1.pkl\n‚úì Saved: models/5_RandomForest_ev_share_t+1.pkl\n‚úì Saved: models/6_LightGBM_ev_share_t+1.pkl\n‚úì Saved: models/1_LinearRegression_ev_share_t+2.pkl\n‚úì Saved: models/2_Ridge_ev_share_t+2.pkl\n‚úì Saved: models/3_Lasso_ev_share_t+2.pkl\n‚úì Saved: models/4_ElasticNet_ev_share_t+2.pkl\n‚úì Saved: models/5_RandomForest_ev_share_t+2.pkl\n‚úì Saved: models/6_LightGBM_ev_share_t+2.pkl\n‚úì Saved: models/1_LinearRegression_ev_share_t+3.pkl\n‚úì Saved: models/2_Ridge_ev_share_t+3.pkl\n‚úì Saved: models/3_Lasso_ev_share_t+3.pkl\n‚úì Saved: models/4_ElasticNet_ev_share_t+3.pkl\n‚úì Saved: models/5_RandomForest_ev_share_t+3.pkl\n‚úì Saved: models/6_LightGBM_ev_share_t+3.pkl\n‚úì Saved: models/scaler.pkl\n‚úì Saved: models/encoders.pkl\n\n============================================================\nSTEP 9: SAVING RESULTS\n============================================================\n‚úì Saved evaluation results: predictions/model_evaluation_results.csv\n‚úì Saved predictions: predictions/predictions_ev_share_t+1.csv\n‚úì Saved predictions: predictions/predictions_ev_share_t+2.csv\n‚úì Saved predictions: predictions/predictions_ev_share_t+3.csv\n\n============================================================\nFINAL PERFORMANCE SUMMARY\n============================================================\n\nev_share_t+1:\n\nModel                       MAE       RMSE         R¬≤       MAPE\n--------------------------------------------------------------\n3_Lasso                0.003649   0.004562    -0.0366      2.74%\nENSEMBLE               0.003693   0.004611    -0.0590      2.77%\n4_ElasticNet           0.003742   0.004633    -0.0692      2.81%\n5_RandomForest         0.003846   0.004773    -0.1349      2.89%\n6_LightGBM             0.003920   0.004908    -0.2001      2.94%\n2_Ridge                0.004167   0.005079    -0.2851      3.14%\n1_LinearRegression     0.004208   0.005202    -0.3481      3.17%\n\nüèÜ BEST: 3_Lasso (MAE: 0.003649)\n\nev_share_t+2:\n\nModel                       MAE       RMSE         R¬≤       MAPE\n--------------------------------------------------------------\n3_Lasso                0.003676   0.004475    -0.0074      2.79%\n4_ElasticNet           0.003685   0.004493    -0.0159      2.80%\nENSEMBLE               0.003713   0.004524    -0.0296      2.82%\n6_LightGBM             0.003972   0.004966    -0.2410      3.02%\n5_RandomForest         0.004133   0.005103    -0.3102      3.14%\n2_Ridge                0.004488   0.005868    -0.7323      3.42%\n1_LinearRegression     0.004772   0.006118    -0.8830      3.64%\n\nüèÜ BEST: 3_Lasso (MAE: 0.003676)\n\nev_share_t+3:\n\nModel                       MAE       RMSE         R¬≤       MAPE\n--------------------------------------------------------------\n3_Lasso                0.003736   0.004680    -0.0021      2.84%\n4_ElasticNet           0.003759   0.004730    -0.0235      2.85%\nENSEMBLE               0.003763   0.004721    -0.0196      2.86%\n5_RandomForest         0.003844   0.004836    -0.0701      2.92%\n6_LightGBM             0.003897   0.004867    -0.0836      2.95%\n2_Ridge                0.004085   0.005115    -0.1970      3.12%\n1_LinearRegression     0.004359   0.005600    -0.4350      3.33%\n\nüèÜ BEST: 3_Lasso (MAE: 0.003736)\n\n============================================================\nTRAINING COMPLETE ‚úÖ\n============================================================\n\nOutputs saved to:\n  Models: models/\n  Predictions: predictions/\n\n============================================================\n","output_type":"stream"}],"execution_count":3}]}